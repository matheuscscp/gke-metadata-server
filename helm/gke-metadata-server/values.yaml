# MIT License
#
# Copyright (c) 2023 Matheus Pimenta
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Default values for gke-metadata-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

config:
  # Mandatory fully-qualified name of the GCP Workload Identity Provider.
  # This full name can be retrieved on the Google Cloud Console webpage for the provider.
  # Must match the pattern: projects/<gcp_project_number>/locations/global/workloadIdentityPools/<pool_name>/providers/<provider_name>
  workloadIdentityProvider: ""
  # Optional Google Service Account email to add on the gke-metadata-server ServiceAccount annotation.
  # This is useful for using the gke-metadata-server with Pods running on the host network (see README.md).
  defaultNodeServiceAccount: ""
  logLevel: info # Log level. Accepted values: panic, fatal, error, warning, info, debug, trace
  serverPort: 8080 # TCP port where the HTTP server will listen on.
  watchPods:
    enable: true # Whether or not to watch and cache the Pods running on the same Node.
    disableFallback: false # Whether or not to disable the simple fallback method for looking up Pods upon cache misses.
    resyncPeriod: 10m # How often to fully resync.
  watchNode:
    enable: true # Whether or not to watch and cache the Node where the server is running.
    disableFallback: false # Whether or not to disable the simple fallback method for looking up the Node upon cache misses.
    resyncPeriod: 1h # How often to fully resync.
  watchServiceAccounts:
    enable: true # Whether or not to watch and cache all the Service Accounts of the cluster.
    disableFallback: false # Whether or not to disable the simple fallback method for looking up Service Accounts upon cache misses.
    resyncPeriod: 1h # How often to fully resync.
  cacheTokens:
    enable: true # Whether or not to proactively cache tokens for the Service Accounts used by the Pods running in the same Node.
    concurrency: 10 # Maximum parallel caching operations.

podAnnotations: {}
  # Optionally, configure Prometheus to scrape the server:
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"
  # prometheus.io/path: /metrics

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

image:
  repository: ghcr.io/matheuscscp/gke-metadata-server
  pullPolicy: IfNotPresent

  # Overrides the image tag whose default is the chart appVersion.
  # tag: some-tag

  # Overrides the image tag whose default is the chart appVersion (stronger than tag).
  # digest: some-digest

# This is a very powerful/critical infrastructure application for granting GCP access to all
# the workloads inside a Kubernetes cluster. It should reside in the kube-system namespace.
# If you want to change this, you must know what you are doing.
namespace: kube-system

# Pod .spec.priorityClassName. This controls how much preemptible the DaemonSet Pods will be.
priorityClass: system-node-critical

# Pod .spec.nodeSelector. This controls in which Nodes the DaemonSet Pods will be scheduled.
nodeSelector: {}

# Pod .spec.affinity. This controls where the DaemonSet Pods will be scheduled, as an
# advanced alternative to nodeSelector.
affinity: {}

# Pod .spec.tolerations. This controls how the DaemonSet Pods will tolerate taints on the Nodes.
tolerations: []
